{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Library","metadata":{}},{"cell_type":"code","source":"# import os\n# import random\n# import numpy as np\n# import pandas as pd\n# from PIL import Image\n# from tqdm.notebook import tqdm\n# from scipy import spatial\n# from sklearn.model_selection import train_test_split\n# import torch\n# from torch import nn\n# from torch.utils.data import Dataset, DataLoader\n# from torch.optim.lr_scheduler import CosineAnnealingLR\n# from torchvision import transforms\n# import timm\n# from timm.utils import AverageMeter\n# import sys\n# sys.path.append('../input/sentence-transformers-222/sentence-transformers')\n# from sentence_transformers import SentenceTransformer\n# import warnings\n# warnings.filterwarnings('ignore')\n\nimport os\nimport random\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nfrom tqdm.notebook import tqdm\nfrom scipy import spatial\nfrom sklearn.model_selection import train_test_split\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nfrom torchvision import transforms, models\nimport sys\nsys.path.append('../input/sentence-transformers-222/sentence-transformers')\nfrom sentence_transformers import SentenceTransformer\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2023-05-07T17:04:25.698387Z","iopub.execute_input":"2023-05-07T17:04:25.699233Z","iopub.status.idle":"2023-05-07T17:04:37.490895Z","shell.execute_reply.started":"2023-05-07T17:04:25.699192Z","shell.execute_reply":"2023-05-07T17:04:37.489779Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# Config","metadata":{}},{"cell_type":"code","source":"# class CFG:\n#     model_name = 'vit_base_patch16_224'\n#     input_size = 224\n#     batch_size = 64\n#     num_epochs = 3\n#     lr = 1e-4\n#     seed = 42\n\nclass CFG:\n    model_name = 'resnet50'\n    input_size = 224\n    batch_size = 64\n    num_epochs = 3\n    lr = 1e-4\n    seed = 42","metadata":{"execution":{"iopub.status.busy":"2023-05-07T17:05:30.710173Z","iopub.execute_input":"2023-05-07T17:05:30.711692Z","iopub.status.idle":"2023-05-07T17:05:30.719107Z","shell.execute_reply.started":"2023-05-07T17:05:30.711642Z","shell.execute_reply":"2023-05-07T17:05:30.718031Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# def seed_everything(seed):\n#     os.environ['PYTHONHASHSEED'] = str(seed)\n#     random.seed(seed)\n#     np.random.seed(seed)\n#     torch.manual_seed(seed)\n    \n#     if torch.cuda.is_available(): \n#         torch.cuda.manual_seed(seed)\n#         torch.backends.cudnn.deterministic = True\n\n\n# seed_everything(CFG.seed)\n\ndef seed_everything(seed):\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    \n    if torch.cuda.is_available(): \n        torch.cuda.manual_seed(seed)\n        torch.backends.cudnn.deterministic = True\n\nseed_everything(CFG.seed)","metadata":{"execution":{"iopub.status.busy":"2023-05-07T17:05:31.560012Z","iopub.execute_input":"2023-05-07T17:05:31.561179Z","iopub.status.idle":"2023-05-07T17:05:31.633854Z","shell.execute_reply.started":"2023-05-07T17:05:31.561133Z","shell.execute_reply":"2023-05-07T17:05:31.632791Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# Dataset","metadata":{}},{"cell_type":"code","source":"class DiffusionDataset(Dataset):\n    def __init__(self, df, transform):\n        self.df = df\n        self.transform = transform\n        \n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        image = Image.open(row['filepath'])\n        image = self.transform(image)\n        prompt = row['prompt']\n        return image, prompt\n    \nclass AverageMeter:\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n\nclass DiffusionCollator:\n    def __init__(self):\n        self.st_model = SentenceTransformer(\n            '/kaggle/input/sentence-transformers-222/all-MiniLM-L6-v2',\n            device='cpu'\n        )\n    \n    def __call__(self, batch):\n        images, prompts = zip(*batch)\n        images = torch.stack(images)\n        prompt_embeddings = self.st_model.encode(\n            prompts, \n            show_progress_bar=False, \n            convert_to_tensor=True\n        )\n        return images, prompt_embeddings\n    \n    \ndef get_dataloaders(\n    trn_df,\n    val_df,\n    input_size,\n    batch_size\n):\n    transform = transforms.Compose([\n        transforms.Resize(input_size),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n    ])\n\n    trn_dataset = DiffusionDataset(trn_df, transform)\n    val_dataset = DiffusionDataset(val_df, transform)\n    collator = DiffusionCollator()\n    \n    dataloaders = {}\n    dataloaders['train'] = DataLoader(\n        dataset=trn_dataset,\n        shuffle=True,\n        batch_size=batch_size,\n        pin_memory=True,\n        num_workers=2,\n        drop_last=True,\n        collate_fn=collator\n    )\n    dataloaders['val'] = DataLoader(\n        dataset=val_dataset,\n        shuffle=False,\n        batch_size=batch_size,\n        pin_memory=True,\n        num_workers=2,\n        drop_last=False,\n        collate_fn=collator\n    )\n    return dataloaders","metadata":{"execution":{"iopub.status.busy":"2023-05-07T17:05:33.258406Z","iopub.execute_input":"2023-05-07T17:05:33.259150Z","iopub.status.idle":"2023-05-07T17:05:33.272670Z","shell.execute_reply.started":"2023-05-07T17:05:33.259109Z","shell.execute_reply":"2023-05-07T17:05:33.271399Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# Train","metadata":{}},{"cell_type":"code","source":"def cosine_similarity(y_trues, y_preds):\n    return np.mean([\n        1 - spatial.distance.cosine(y_true, y_pred) \n        for y_true, y_pred in zip(y_trues, y_preds)\n    ])","metadata":{"execution":{"iopub.status.busy":"2023-05-07T17:05:34.407433Z","iopub.execute_input":"2023-05-07T17:05:34.408163Z","iopub.status.idle":"2023-05-07T17:05:34.413883Z","shell.execute_reply.started":"2023-05-07T17:05:34.408125Z","shell.execute_reply":"2023-05-07T17:05:34.412464Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# def train(\n#     trn_df,\n#     val_df,\n#     model_name,\n#     input_size,\n#     batch_size,\n#     num_epochs,\n#     lr\n# ):\n#     device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n#     dataloaders = get_dataloaders(\n#         trn_df,\n#         val_df,\n#         input_size,\n#         batch_size\n#     )\n\n#     model = timm.create_model(\n#         model_name,\n#         pretrained=True,\n#         num_classes=384\n#     )\n#     model.set_grad_checkpointing()\n#     model.to(device)\n    \n#     optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n\n#     ttl_iters = num_epochs * len(dataloaders['train'])\n#     scheduler = CosineAnnealingLR(optimizer, T_max=ttl_iters, eta_min=1e-6)\n#     criterion = nn.CosineEmbeddingLoss()\n    \n#     best_score = -1.0\n\n#     for epoch in range(num_epochs):\n#         train_meters = {\n#             'loss': AverageMeter(),\n#             'cos': AverageMeter(),\n#         }\n#         model.train()\n#         for X, y in tqdm(dataloaders['train'], leave=False):\n#             X, y = X.to(device), y.to(device)\n\n#             optimizer.zero_grad()\n#             X_out = model(X)\n#             target = torch.ones(X.size(0)).to(device)\n#             loss = criterion(X_out, y, target)\n#             loss.backward()\n\n#             optimizer.step()\n#             scheduler.step()\n\n#             trn_loss = loss.item()\n#             trn_cos = cosine_similarity(\n#                 X_out.detach().cpu().numpy(), \n#                 y.detach().cpu().numpy()\n#             )\n\n#             train_meters['loss'].update(trn_loss, n=X.size(0))\n#             train_meters['cos'].update(trn_cos, n=X.size(0))\n\n#         print('Epoch {:d} / trn/loss={:.4f}, trn/cos={:.4f}'.format(\n#             epoch + 1,\n#             train_meters['loss'].avg,\n#             train_meters['cos'].avg))\n\n#         val_meters = {\n#             'loss': AverageMeter(),\n#             'cos': AverageMeter(),\n#         }\n#         model.eval()\n#         for X, y in tqdm(dataloaders['val'], leave=False):\n#             X, y = X.to(device), y.to(device)\n\n#             with torch.no_grad():\n#                 X_out = model(X)\n#                 target = torch.ones(X.size(0)).to(device)\n#                 loss = criterion(X_out, y, target)\n\n#                 val_loss = loss.item()\n#                 val_cos = cosine_similarity(\n#                     X_out.detach().cpu().numpy(), \n#                     y.detach().cpu().numpy()\n#                 )\n\n#             val_meters['loss'].update(val_loss, n=X.size(0))\n#             val_meters['cos'].update(val_cos, n=X.size(0))\n\n#         print('Epoch {:d} / val/loss={:.4f}, val/cos={:.4f}'.format(\n#             epoch + 1,\n#             val_meters['loss'].avg,\n#             val_meters['cos'].avg))\n        \n#         if val_meters['cos'].avg > best_score:\n#             best_score = val_meters['cos'].avg\n#             torch.save(model.state_dict(), f'{model_name}.pth')\n\n\n\n\ndef train(\n    trn_df,\n    val_df,\n    model_name,\n    input_size,\n    batch_size,\n    num_epochs,\n    lr\n):\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    dataloaders = get_dataloaders(\n        trn_df,\n        val_df,\n        input_size,\n        batch_size\n    )\n    \n    model = models.resnet50(pretrained=True)\n    num_ftrs = model.fc.in_features\n    model.fc = nn.Linear(num_ftrs, 384)\n    model.to(device)\n    \n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n\n    ttl_iters = num_epochs * len(dataloaders['train'])\n    scheduler = CosineAnnealingLR(optimizer, T_max=ttl_iters, eta_min=1e-6)\n    criterion = nn.CosineEmbeddingLoss()\n\n    best_score = -1.0\n    \n    for epoch in range(num_epochs):\n        train_meters = {\n        'loss': AverageMeter(),\n        'cos': AverageMeter(),\n    }\n        model.train()\n        for X, y in tqdm(dataloaders['train'], leave=False):\n            X, y = X.to(device), y.to(device)\n            optimizer.zero_grad()\n            X_out = model(X)\n            target = torch.ones(X.size(0)).to(device)\n            loss = criterion(X_out, y, target)\n            loss.backward()\n            optimizer.step()\n            scheduler.step()\n\n            trn_loss = loss.item()\n            trn_cos = cosine_similarity(\n                X_out.detach().cpu().numpy(), \n                y.detach().cpu().numpy()\n            )\n\n            train_meters['loss'].update(trn_loss, n=X.size(0))\n            train_meters['cos'].update(trn_cos, n=X.size(0))\n        print('Epoch {:d} / trn/loss={:.4f}, trn/cos={:.4f}'.format(\n        epoch + 1,\n        train_meters['loss'].avg,\n        train_meters['cos'].avg))\n        \n        val_meters = {\n        'loss': AverageMeter(),\n        'cos': AverageMeter(),\n        }\n        model.eval()\n        for X, y in tqdm(dataloaders['val'], leave=False):\n            X, y = X.to(device), y.to(device)\n            \n            with torch.no_grad():\n                X_out = model(X)\n                target = torch.ones(X.size(0)).to(device)\n                loss = criterion(X_out, y, target)\n                \n                val_loss = loss.item()\n                val_cos = cosine_similarity(\n                    X_out.detach().cpu().numpy(), \n                    y.detach().cpu().numpy()\n                )\n            val_meters['loss'].update(val_loss, n=X.size(0))\n            val_meters['cos'].update(val_cos, n=X.size(0))\n            \n        print('Epoch {:d} / val/loss={:.4f}, val/cos={:.4f}'.format(\n            epoch + 1,\n            val_meters['loss'].avg,\n            val_meters['cos'].avg))\n        if val_meters['cos'].avg > best_score:\n            best_score = val_meters['cos'].avg\n            torch.save(model.state_dict(), f'{model_name}.pth')\n            \n    return model\n\n","metadata":{"execution":{"iopub.status.busy":"2023-05-07T21:10:19.049740Z","iopub.execute_input":"2023-05-07T21:10:19.050199Z","iopub.status.idle":"2023-05-07T21:10:19.068561Z","shell.execute_reply.started":"2023-05-07T21:10:19.050160Z","shell.execute_reply":"2023-05-07T21:10:19.067398Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# df = pd.read_csv('/kaggle/input/diffusiondb-data-cleansing/diffusiondb.csv')\n# trn_df, val_df = train_test_split(df, test_size=0.1, random_state=CFG.seed)\n\ndf = pd.read_csv('/kaggle/input/diffusiondb-data-cleansing/diffusiondb.csv')\ntrn_df, val_df = train_test_split(df, test_size=0.1, random_state=CFG.seed)\n\n","metadata":{"execution":{"iopub.status.busy":"2023-05-07T21:10:21.896872Z","iopub.execute_input":"2023-05-07T21:10:21.897669Z","iopub.status.idle":"2023-05-07T21:10:22.665302Z","shell.execute_reply.started":"2023-05-07T21:10:21.897624Z","shell.execute_reply":"2023-05-07T21:10:22.664239Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"print(model)","metadata":{"execution":{"iopub.status.busy":"2023-05-07T21:10:58.133749Z","iopub.execute_input":"2023-05-07T21:10:58.134486Z","iopub.status.idle":"2023-05-07T21:10:58.140399Z","shell.execute_reply.started":"2023-05-07T21:10:58.134440Z","shell.execute_reply":"2023-05-07T21:10:58.139280Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"None\n","output_type":"stream"}]},{"cell_type":"code","source":"# train(trn_df, val_df, CFG.model_name, CFG.input_size, CFG.batch_size, CFG.num_epochs, CFG.lr)\n\nmodel = train(trn_df, val_df, CFG.model_name, CFG.input_size, CFG.batch_size, CFG.num_epochs, CFG.lr)","metadata":{"execution":{"iopub.status.busy":"2023-05-07T21:11:02.485346Z","iopub.execute_input":"2023-05-07T21:11:02.486466Z","iopub.status.idle":"2023-05-08T00:44:28.857249Z","shell.execute_reply.started":"2023-05-07T21:11:02.486417Z","shell.execute_reply":"2023-05-08T00:44:28.856024Z"},"trusted":true},"execution_count":16,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2170 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 1 / trn/loss=0.5137, trn/cos=0.4863\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/242 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 1 / val/loss=0.4878, val/cos=0.5122\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2170 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 2 / trn/loss=0.4644, trn/cos=0.5356\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/242 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 2 / val/loss=0.4697, val/cos=0.5303\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2170 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 3 / trn/loss=0.4316, trn/cos=0.5684\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/242 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 3 / val/loss=0.4674, val/cos=0.5326\n","output_type":"stream"}]},{"cell_type":"code","source":"model_path = '/kaggle/working/model.pth'\ntorch.save(model.state_dict(), model_path)","metadata":{"execution":{"iopub.status.busy":"2023-05-08T01:05:51.007880Z","iopub.execute_input":"2023-05-08T01:05:51.009116Z","iopub.status.idle":"2023-05-08T01:05:51.181459Z","shell.execute_reply.started":"2023-05-08T01:05:51.009056Z","shell.execute_reply":"2023-05-08T01:05:51.180358Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom pathlib import Path\nfrom PIL import Image\nfrom tqdm.notebook import tqdm\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nimport timm","metadata":{"execution":{"iopub.status.busy":"2023-05-08T01:12:03.563552Z","iopub.execute_input":"2023-05-08T01:12:03.564466Z","iopub.status.idle":"2023-05-08T01:12:03.858003Z","shell.execute_reply.started":"2023-05-08T01:12:03.564425Z","shell.execute_reply":"2023-05-08T01:12:03.856739Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"class CFG:\n    model_path = '/kaggle/working/resnet50.pth'\n    model_name = 'resnet50'\n    input_size = 224\n    batch_size = 64","metadata":{"execution":{"iopub.status.busy":"2023-05-08T01:13:53.838345Z","iopub.execute_input":"2023-05-08T01:13:53.839341Z","iopub.status.idle":"2023-05-08T01:13:53.844617Z","shell.execute_reply.started":"2023-05-08T01:13:53.839301Z","shell.execute_reply":"2023-05-08T01:13:53.843341Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"class DiffusionTestDataset(Dataset):\n    def __init__(self, images, transform):\n        self.images = images\n        self.transform = transform\n    \n    def __len__(self):\n        return len(self.images)\n\n    def __getitem__(self, idx):\n        image = Image.open(self.images[idx])\n        image = self.transform(image)\n        return image","metadata":{"execution":{"iopub.status.busy":"2023-05-08T01:13:55.634697Z","iopub.execute_input":"2023-05-08T01:13:55.635395Z","iopub.status.idle":"2023-05-08T01:13:55.641825Z","shell.execute_reply.started":"2023-05-08T01:13:55.635358Z","shell.execute_reply":"2023-05-08T01:13:55.640571Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"def predict(\n    images,\n    model_path,\n    model_name,\n    input_size,\n    batch_size\n):\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    transform = transforms.Compose([\n        transforms.Resize(input_size),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n    ])\n    dataset = DiffusionTestDataset(images, transform)\n    dataloader = DataLoader(\n        dataset=dataset,\n        shuffle=False,\n        batch_size=batch_size,\n        pin_memory=True,\n        num_workers=2,\n        drop_last=False\n    )\n\n    model = timm.create_model(\n        model_name,\n        pretrained=False,\n        num_classes=384\n    )\n    state_dict = torch.load(model_path)\n    model.load_state_dict(state_dict)\n    model.to(device)\n    model.eval()\n    \n    preds = []\n    for X in tqdm(dataloader, leave=False):\n        X = X.to(device)\n\n        with torch.no_grad():\n            X_out = model(X)\n            preds.append(X_out.cpu().numpy())\n    \n    return np.vstack(preds).flatten()","metadata":{"execution":{"iopub.status.busy":"2023-05-08T01:14:24.749489Z","iopub.execute_input":"2023-05-08T01:14:24.749882Z","iopub.status.idle":"2023-05-08T01:14:24.759400Z","shell.execute_reply.started":"2023-05-08T01:14:24.749847Z","shell.execute_reply":"2023-05-08T01:14:24.758064Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"images = list(Path('/kaggle/input/stable-diffusion-image-to-prompts/images').glob('*.png'))\nimgIds = [i.stem for i in images]\nEMBEDDING_LENGTH = 384\nimgId_eId = [\n    '_'.join(map(str, i)) for i in zip(\n        np.repeat(imgIds, EMBEDDING_LENGTH),\n        np.tile(range(EMBEDDING_LENGTH), len(imgIds)))]\n\nprompt_embeddings = predict(images, CFG.model_path, CFG.model_name, CFG.input_size, CFG.batch_size)\nsubmission = pd.DataFrame(\n    index=imgId_eId,\n    data=prompt_embeddings,\n    columns=['val']\n).rename_axis('imgId_eId')\nsubmission.to_csv('submission.csv')","metadata":{"execution":{"iopub.status.busy":"2023-05-08T01:14:54.235283Z","iopub.execute_input":"2023-05-08T01:14:54.236358Z","iopub.status.idle":"2023-05-08T01:14:55.310695Z","shell.execute_reply.started":"2023-05-08T01:14:54.236319Z","shell.execute_reply":"2023-05-08T01:14:55.309425Z"},"trusted":true},"execution_count":22,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}